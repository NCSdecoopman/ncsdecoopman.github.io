[
  {
    "objectID": "projets/missingdatalab/missingdatalab.html",
    "href": "projets/missingdatalab/missingdatalab.html",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "",
    "text": "Qui n‚Äôa jamais eu ValueError: NaN lors d‚Äôune analyse de donn√©es. L‚Äô√©tape d‚Äôexploration commence souvent avec une analyse des donn√©es manquantes. Une donn√©e manquante correspond √† l‚Äôabsence d‚Äôune valeur dans un ensemble de donn√©es, l√† o√π une information est normalement attendue. Cela peut √™tre d√ª √† une erreur de collecte de la donn√©e ou une non-r√©ponse lors d‚Äôenqu√™te. Probl√®me : un isna() se r√©v√®le souvent positif. On r√©alise alors un dropna() et le probl√®me disparait ! On peut mieux faire."
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#contexte-et-probl√©matique",
    "href": "projets/missingdatalab/missingdatalab.html#contexte-et-probl√©matique",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "1 Contexte et probl√©matique",
    "text": "1 Contexte et probl√©matique\nLa pr√©sence de donn√©es manquantes peut pertuber les analyses, engendrer une perte d‚Äôinformations, introduire des biais et r√©duire les performances de mod√©lisations. Il faut alors comprendre les m√©canismes de manquement des donn√©es, adapter la m√©thode de collecte lorsque c‚Äôest possible et imputer les donn√©es manquantes.\nCe projet r√©alis√© en groupe pendant l‚Äôann√©e de M2 SSD √† l‚ÄôUGA s‚Äôattache √† savoir comment mettre en ≈ìuvre des strat√©gies d‚Äôimputation efficaces pour diff√©rents types de donn√©es manquantes : MCAR, MAR, MNAR ? Les objectifs √©tant de maintenir la validit√© d‚Äôune analyse, d‚Äôam√©liorer la performance et la fiabilit√© des mod√®les et d‚Äô√©viter des conclusions erron√©es."
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#donn√©es",
    "href": "projets/missingdatalab/missingdatalab.html#donn√©es",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "2 Donn√©es",
    "text": "2 Donn√©es\nLes donn√©es utilis√©es proviennent de Kaggle. Il s‚Äôagit d‚Äôun questionnaire concernant les comportements li√©s √† la consommation de cigarettes, compos√© de 14 variables et de 1000 individus. Parmi ces variables, on trouve des informations telles que le nombre de cigarettes consomm√©es par jour, le nombre d‚Äôexp√©riences li√©es √† la drogue, ainsi que des variables explicatives comme le groupe d‚Äô√¢ge, le statut socio-√©conomique, et l‚Äôacc√®s ou non √† une aide pour arr√™ter de fumer. Un jeu de donn√©es semblable a √©t√© simul√© √† l‚Äôaide de la biblioth√®que simstudy (1). Ce jeu de donn√©es int√®gre des corr√©lations entre les variables. Cette approche permet d‚Äôexplorer des sc√©narios vari√©s et proches de la r√©alit√© :\nTrois m√©canismes li√©s √† la variable repr√©sentant l‚Äôacc√®s √† de l‚Äôaide :\n\nune goutte d‚Äôeau rend illisible une r√©ponse\nles personnes qui continuent √† fumer cachent avoir eu de l‚Äôaide\nles personnes ayant eu de l‚Äôaide le cachent\n\nEnsuite, trois m√©canismes li√©s √† la pr√©valence tabagique :\n\nun bug r√©seau emp√™che l‚Äôenregistrement de la r√©ponse\nles personnes jeunes cachent le fait de fumer\nles personnes fumant beaucoup cachent leur consommation tagabique"
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#approche-et-m√©thologie",
    "href": "projets/missingdatalab/missingdatalab.html#approche-et-m√©thologie",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "3 Approche et m√©thologie",
    "text": "3 Approche et m√©thologie\n\n3.1 Typologie des donn√©es manquantes\nLes donn√©es manquantes peuvent √™tre class√©es en trois cat√©gories principales, selon le m√©canisme qui g√©n√®re leur absence. Cette classification est essentielle pour choisir la m√©thode de traitement adapt√©e.\n\n3.1.1 Donn√©es manquantes compl√®tement al√©atoires (MCAR)\nLa probabilit√© que la donn√©e soit manquante est ind√©pendante de toute autre variable, observ√©e ou non observ√©e. Cela signifie que l‚Äôabsence d‚Äôune donn√©e est totalement al√©atoire et ne d√©pend d‚Äôaucune information contenue dans le jeu de donn√©es (2).\nExemple : lors d‚Äôun questionnaire papier, certaines r√©ponses peuvent √™tre perdues √† cause d‚Äôun probl√®me d‚Äôimpression. Ici, l‚Äôabsence de r√©ponse ne d√©pend ni de la question pos√©e ni des caract√©ristiques des r√©pondants, ce qui signifie que les donn√©es sont absentes de mani√®re totalement al√©atoire.\nFormule math√©matique : d√©crite en annexe.\n\n\n3.1.2 Donn√©es manquantes al√©atoires (MAR)\nLa probabilit√© que la donn√©e soit manquante d√©pend uniquement des variables observ√©es, mais pas de la valeur manquante elle-m√™me. Autrement dit, l‚Äôabsence d‚Äôune valeur est expliqu√©e par des variables que l‚Äôon peut observer, mais pas par la valeur manquante en elle-m√™me (3).\nExemple : dans une enqu√™te sur l‚Äô√©tat de sant√©, il se peut que les personnes plus √¢g√©es r√©pondent moins souvent aux questions sur leur activit√© physique. Ici, l‚Äôabsence de r√©ponse d√©pend d‚Äôune variable observ√©e (l‚Äô√¢ge), mais pas directement du niveau d‚Äôactivit√© physique de la personne.\nFormule math√©matique : d√©crite en annexe.\n\n\n3.1.3 Donn√©es manquantes non al√©atoires (MNAR)\nLa probabilit√© que la donn√©e soit manquante d√©pend au moins de la valeur de la donn√©e elle-m√™me. Cela signifie que l‚Äôabsence d‚Äôune donn√©e est directement li√©e √† sa propre valeur. √Ä noter qu‚Äôelle peut tout √† fait d√©pendre √©galement de variables observ√©es (ou non observ√©es). Autrement dit, une donn√©e manquante de type MNAR se trouve √™tre toute donn√©e manquante n‚Äô√©tant pas de type MAR ou MCAR.\nExemple : lors d‚Äôune enqu√™te sur la consommation d‚Äôalcool chez les jeunes, les personnes ayant une consommation excessive peuvent √™tre plus enclines √† ne pas r√©pondre √† la question, par peur du jugement. Dans ce cas, l‚Äôabsence de r√©ponse est directement li√©e √† la valeur de la variable, ce qui rend le traitement de ces donn√©es plus complexe.\nFormule math√©matique : d√©crite en annexe.\n\n\n\n3.2 G√©n√©ration des donn√©es manquantes\nPour g√©n√©rer la probabilit√© d‚Äôabsence d‚Äôune observation, on utilise une fonction sigmo√Øde qui transforme les valeurs en probabilit√©s entre 0 et 1 d√©crite en annexe. Pour atteintre une proportion cible de donn√©es manquantes, on optimise les param√®tres d‚Äôajustement en minimisant l‚Äô√©cart entre la moyenne des probabilit√©s pr√©dites et cette proportion cible (4), en r√©solvant l‚Äô√©quation d√©crite en annexe.\n\n\n3.3 Algorithmes d‚Äôimputations\nPour remplacer les valeurs manquantes dans les jeux de donn√©es plusieurs m√©thodes sont utilis√©es. Ces m√©thodes vont des plus simples aux approches plus complexes.\n\n3.3.1 Imputation simple par une valeur statistique\nLa moyenne consiste √† remplacer les valeurs manquantes par la moyenne des valeurs observ√©es pour la variable concern√©e (2). La m√©diane est une m√©thode robuste aux valeurs extr√™mes. Elle remplace les valeurs manquantes par la m√©diane des valeurs observ√©es (5). Le mode est adapt√©e aux variables cat√©gorielles (6). Elle remplace les valeurs manquantes par la modalit√© la plus fr√©quente. La valeur constante consiste √† remplacer les valeurs manquantes par une valeur pr√©d√©finie (7) (ex : 0, -999, etc.). Les formules sont √©crites en annexe.\n\n\n3.3.2 Imputation bas√©e sur les voisins : K-Nearest Neighbors (KNN)\nL‚Äôimputation par KNN repose sur la similarit√© entre les observations (8). Pour chaque valeur manquante, les k plus proches voisins sont identifi√©s et la valeur manquante est estim√©e en utilisant leurs valeurs. La formule pour les variables num√©riques est en annexe. Pour les variables cat√©gorielles, la valeur la plus fr√©quente parmi les voisins est choisie.\n\n\n3.3.3 Imputation par mod√®les avanc√©s\n\n3.3.3.1 SoftImputer\nSoftImputer est une m√©thode d‚Äôimputation des valeurs manquantes bas√©e sur la factorisation de matrices (9). Son principe repose sur l‚Äôestimation d‚Äôune matrice compl√®te en effectuant une d√©composition en valeurs singuli√®res (SVD) tout en appliquant un seuillage doux sur les valeurs singuli√®res pour favoriser la parcimonie. L‚Äôalgorithme est d√©taill√© en annexe. Cette m√©thode est efficace pour capturer la structure sous-jacente des donn√©es tout en limitant le surajustement.\n\n\n3.3.3.2 ACP\nmissMDA utilise des techniques de r√©duction de dimension telles que l‚ÄôACP pour imputer les valeurs manquantes (10). Cette m√©thode est particuli√®rement adapt√©e aux donn√©es continues. L‚Äôalgorithme est d√©taill√© en annexe.\n\n\n3.3.3.3 Imputation par [M]ICE ([Multiple] Imputation by Chained Equations)\nMICE r√©alise des imputations multiples par √©quations cha√Æn√©es (11). Chaque variable avec des valeurs manquantes est mod√©lis√©e en fonction des autres variables de mani√®re it√©rative. Ici on √©tudiera une seule imputation g√©n√©r√©e afin de simplifier l‚Äôanalyse. La technique sera donc nomm√©e ICE et d√©crite en annexe.\n\n\n3.3.3.4 Imputation par MissForest\nMissForest est une m√©thode non param√©trique bas√©e sur les for√™ts al√©atoires (9). Elle impute les valeurs manquantes en construisant un mod√®le de pr√©diction pour chaque variable contenant des valeurs manquantes. √Ä chaque it√©ration, les for√™ts al√©atoires sont utilis√©es pour pr√©dire les valeurs manquantes en utilisant les autres variables comme pr√©dicteurs (algorithmique).\n\n\n\n\n3.4 Plan d‚Äô√©valuation\n\n3.4.1 G√©n√©ration des donn√©es manquantes\nOn commence par g√©n√©rer les donn√©es manquantes suivant les trois types de m√©canismes (MCAR, MAR et MNAR) et suivant diff√©rentes proportions : 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.8. Nous r√©p√©tons ce processus 100 fois.\n\n\n3.4.2 Optimisation des m√©thodes\nUne fois les donn√©es g√©n√©r√©es, nous appliquons les diff√©rentes m√©thodes d‚Äôimputation. Ces m√©thodes utilisent des approches vari√©es et n√©cessitent souvent des hyperparam√®tres pour optimiser leurs performances. Pour chaque m√©thode, nous explorons diff√©rentes combinaisons d‚Äôhyperparam√®tres √† l‚Äôaide d‚Äôune recherche par grille (Grid Search). Notre objectif est de trouver la combinaison qui maximise l‚Äôexactitude (accuracy) pour les variables cat√©gorielles et minimise l‚Äôerreur quadratique moyenne (MSE) pour les variables num√©riques. Pour √©viter le surajustement, nous utilisons une validation crois√©e en 5 sous-√©chantillons (K-Fold Cross Validation). Nous divisons le jeu de donn√©es en 5 groupes, et chaque configuration d‚Äôhyperparam√®tres est test√©e sur 4 groupes et valid√©e sur le 5√®me. Nous calculons la moyenne des performances pour les meilleurs hyperparam√®tres d√©termin√©s.\n\n\n3.4.3 Imputation\nUne fois les m√©thodes optimis√©es, nous les appliquons √† l‚Äôensemble des donn√©es manquantes g√©n√©r√©es 10 fois et nous calculons la moyenne des performances et l‚Äô√©cart type associ√©.\n\n\n3.4.4 Crit√®res d‚Äô√©valuation\nNous cherchons √† d√©termine l‚Äôinfluence du taux de donn√©es manquantes suivant les diff√©rents m√©canismes sur les m√©thodes d‚Äôimputations optimis√©es via l‚Äôexactitude (variable cat√©gorielle) ou la MSE (variable num√©rique), le temps d‚Äôex√©cution de l‚Äôimputation, le pic de m√©moire atteint lors de l‚Äôimputation et la variabilit√© des valeurs imput√©es."
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#r√©sultats",
    "href": "projets/missingdatalab/missingdatalab.html#r√©sultats",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "4 R√©sultats",
    "text": "4 R√©sultats\n\n4.1 Bruts\nLes m√©thodes d‚Äôimputation des variables cat√©gorielles montrent que MissForest offre les meilleures performances, suivie par ICE et KNN, tandis que les m√©thodes plus simples sont les moins performantes. En termes de rapidit√© et d‚Äôutilisation m√©moire, toutes les m√©thodes, sauf MissForest, sont efficaces, bien que KNN soit plus co√ªteuse en m√©moire. De plus, les m√©thodes simples sont particuli√®rement sensibles √† la proportion de donn√©es manquantes, ce qui peut d√©grader leur efficacit√©, tandis que les autres techniques sont plus robustes face √† cette variabilit√©.\nPour les variables num√©riques, ICE, KNN et MissForest offrent les meilleures performances d‚Äôimputation, alors que l‚ÄôACP se r√©v√®le moins efficace. Toutefois, si l‚Äôon consid√®re le temps d‚Äôex√©cution, les m√©thodes simples et KNN sont plus rapides, contrairement √† MissForest. En termes de consommation m√©moire, seule KNN pose probl√®me, les autres √©tant plus l√©g√®res. Enfin, la sensibilit√© aux m√©canismes de donn√©es manquantes montre que les m√©thodes simples sont sensibles aux performances, tandis que SoftImputer est plus gourmand en temps d‚Äôex√©cution lorsqu‚Äôil est confront√© √† ces variations.\n\nVar. cat√©gorielleVar. num√©rique\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.2 R√©capitulatifs\nChaque m√©thode a ses forces et ses faiblesses : les approches simples sont rapides et faciles √† mettre en ≈ìuvre, mais elles peuvent manquer de pr√©cision. √Ä l‚Äôinverse, les m√©thodes avanc√©es offrent de meilleurs r√©sultats, mais elles sont plus gourmandes en temps et en ressources. Nos tests ont montr√© que le choix de la m√©thode d√©pend fortement du type de donn√©es manquantes et du contexte d‚Äôanalyse. Par exemple, pour des donn√©es MCAR ou MAR, des techniques comme KNN ou MissForest fonctionnent bien. En revanche, pour des donn√©es MNAR (qui sont plus complexes √† g√©rer), des m√©thodes comme MissForest ou ICE s‚Äôav√®rent plus efficaces. Cela dit, ces derni√®res demandent davantage de calculs et d‚Äôefforts. Voici un tableau r√©capitulatif des tr√®s bonnes (X) ou bonnes (x) performances et robustesses de chaque m√©thode :\n\n\n\n\n\n\n\n\n\n\n\nM√©thode\nQualit√© d‚Äôimputation\nTemps d‚Äô√©x√©cution\nRessources\nProportion de NA\nM√©canisme de NA\n\n\n\n\nSimple\n\nX\nX\n\n\n\n\nKNN\nx\nX\n\n\nX\n\n\nSoftImputer\n\nx\nX\n\n\n\n\nACP\nx\nx\nX\n\nX\n\n\nICE\nX\nx\nX\nX\nX\n\n\nMissForest\nX\n\nX\nX\nX\n\n\n\nN√©anmoins, bien qu‚Äôil nous ait √©t√© permis de faire ressortir ces informations par le biais de cette √©tude, ces derni√®res ne sont malheuresement que difficilement exploitable. En effet, il s‚Äôav√®re √™tre une t√¢che ardue, pour ne pas dire impossible, de reconna√Ætre le type de donn√©es manquantes sur des donn√©es r√©elles. S‚Äôil existe bien quelques tests statistiques afin d‚Äôidentifier si les donn√©es sont de type MCAR (12), il n‚Äôexiste pas de r√©elle m√©thode pour permettre de d√©celer des donn√©es manquantes MAR ou MNAR."
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#technologies-utilis√©es",
    "href": "projets/missingdatalab/missingdatalab.html#technologies-utilis√©es",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "5 Technologies utilis√©es",
    "text": "5 Technologies utilis√©es\nNous n‚Äôavons pas cherch√© √† innover ou √† utiliser les derni√®res tendances technologiques, mais plut√¥t √† consolider nos acquis en nous appuyant sur des outils √©prouv√©s et ma√Ætris√©s. Le langage principal utilis√© est Python, avec R pour la g√©n√©ration des donn√©es. Nous avons privil√©gi√© des biblioth√®ques bien √©tablies pour l‚Äôanalyse et la manipulation des donn√©es (NumPy (13), Pandas (14), SciPy (15)), la visualisation (Matplotlib (16), Seaborn (17)) et le machine learning (Scikit-learn (18), Joblib (19), Threadpoolctl (20)), garantissant ainsi une approche fiable et reproductible."
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#difficult√©s-et-le√ßons-apprises",
    "href": "projets/missingdatalab/missingdatalab.html#difficult√©s-et-le√ßons-apprises",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "6 Difficult√©s et le√ßons apprises",
    "text": "6 Difficult√©s et le√ßons apprises\n\nCompatibilit√© avec les biblioth√®ques existantes\n\n\n\nIl est crucial de bien d√©finir les formats de donn√©es en entr√©e et en sortie pour assurer une compatibilit√© fluide avec NumPy, Pandas et Scikit-learn.\n\nStandardiser les formats d‚Äôentr√©e (ex. DataFrame vs array) √©vite des erreurs lors du passage entre diff√©rentes m√©thodes d‚Äôimputation.\n\n\nUniformisation des entr√©es et sorties\n\n\n\nChaque m√©thode ayant ses propres exigences en mati√®re de format d‚Äôentr√©e, un pr√©traitement uniforme doit √™tre mis en place pour √©viter les erreurs.\n\nUne normalisation pr√©alable peut am√©liorer la coh√©rence des imputations (ex. standardisation des valeurs num√©riques).\n\n\nStructuration des m√©thodes d‚Äôimputation sous forme de classes\n\n\n\nLa modularisation sous forme de classes permet une meilleure r√©utilisation et lisibilit√© du code.\n\nIl est pr√©f√©rable d‚Äôh√©riter d‚Äôune classe m√®re contenant des fonctionnalit√©s communes.\n\n\nValidation rigoureuse des imputations\n\n\n\nMasquer artificiellement des valeurs et comparer l‚Äôimputation avec l‚Äôobservation r√©elle est une approche fiable pour √©valuer la qualit√© des m√©thodes.\nAssurer une s√©paration correcte entre les ensembles d‚Äôentra√Ænement et de validation pour √©viter les fuites de donn√©es.\n\n\nTraitement efficace des donn√©es massives\n\n\n\nL‚Äôoptimisation des calculs est essentielle pour appliquer des m√©thodes d‚Äôimputation sur de grands jeux de donn√©es.\n\nRecourir √† des impl√©mentations vectoris√©es, au multiprocessing ou √† des approches incr√©mentales peut r√©duire le temps d‚Äôex√©cution."
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#lien-vers-le-projet",
    "href": "projets/missingdatalab/missingdatalab.html#lien-vers-le-projet",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "7 Lien vers le projet",
    "text": "7 Lien vers le projet\nLe code source est accessible sur GitHub : https://github.com/NCSdecoopman/MissingDataLab."
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#cas-dusage-et-perspectives",
    "href": "projets/missingdatalab/missingdatalab.html#cas-dusage-et-perspectives",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "8 Cas d‚Äôusage et perspectives",
    "text": "8 Cas d‚Äôusage et perspectives\nLes donn√©es manquantes biaisent les analyses et les mod√®les d‚ÄôIA, rendant essentielle l‚Äôidentification de leur type (MCAR, MAR, MNAR). Un outil d√©di√© permettrait d‚Äôanalyser ces motifs, puis d‚Äôadapter l‚Äôimputation en cons√©quence. L‚Äôobjectif est d‚Äôint√©grer des mod√®les hybrides pour imputer intelligemment les valeurs manquantes selon leur nature. Automatiser ce processus am√©liorerait la pr√©cision des mod√®les tout en r√©duisant l‚Äôintervention humaine, avec des applications cl√©s en m√©t√©o, sant√© et finance."
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#r√©f√©rences",
    "href": "projets/missingdatalab/missingdatalab.html#r√©f√©rences",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "9 R√©f√©rences",
    "text": "9 R√©f√©rences\n\n\n1. Goldfeld K. simstudy: Simulation of Study Data [Internet]. 2021. Disponible sur: https://cran.r-project.org/web/packages/simstudy/\n\n\n2. Rubin DB. Inference and missing data. Biometrika [Internet]. 1976;63(3):581‚Äë92. Disponible sur: https://doi.org/10.1093/biomet/63.3.581\n\n\n3. Little RJ, Rubin DB. Statistical analysis with missing data. Journal of the American Statistical Association [Internet]. 1987;82(397):87‚Äë99. Disponible sur: https://doi.org/10.2307/2289136\n\n\n4. Van Buuren S. Flexible Imputation of Missing Data [Internet]. Chapman; Hall/CRC; 2018. Disponible sur: https://doi.org/10.1201/9780429492259\n\n\n5. Tukey JW. Exploratory Data Analysis [Internet]. Addison-Wesley; 1977. Disponible sur: https://www.worldcat.org/title/13580185\n\n\n6. Mosteller F, Tukey JW. Data Analysis and Regression [Internet]. Addison-Wesley; 1977. Disponible sur: https://www.worldcat.org/title/1896078\n\n\n7. Schafer JL. Analysis of incomplete multivariate data. Chapman and Hall/CRC [Internet]. 1997; Disponible sur: https://doi.org/10.1201/9781439821862\n\n\n8. Cover TM, Hart PE. Nearest neighbor pattern classification. IEEE Transactions on Information Theory [Internet]. 1967;13(1):21‚Äë7. Disponible sur: https://doi.org/10.1109/TIT.1967.1053964\n\n\n9. Mazumder R, Hastie T, Tibshirani R. Spectral regularization algorithms for learning large incomplete matrices. Journal of Machine Learning Research [Internet]. 2010;11:2287‚Äë322. Disponible sur: http://www.jmlr.org/papers/v11/mazumder10a.html\n\n\n10. Josse J, Husson F. missMDA: a package for handling missing values in multivariate data analysis. Journal of Statistical Software [Internet]. 2016;70:1‚Äë31. Disponible sur: https://doi.org/10.18637/jss.v070.i01\n\n\n11. Rubin DB. Multiple Imputation for Nonresponse in Surveys [Internet]. John Wiley & Sons; 1987. Disponible sur: https://doi.org/10.1002/9780470316696\n\n\n12. Little RJA. A Test of Missing Completely at Random for Multivariate Data with Missing Values. Journal of the American Statistical Association [Internet]. 1988;83(404):1198‚Äë202. Disponible sur: https://doi.org/10.1080/01621459.1988.10478722\n\n\n13. Harris CR et al. Array programming with NumPy. Nature [Internet]. 2020;585:357‚Äë62. Disponible sur: https://doi.org/10.1038/s41586-020-2649-2\n\n\n14. McKinney W. Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference [Internet]. 2010;445:51‚Äë6. Disponible sur: https://doi.org/10.25080/Majora-92bf1922-00a\n\n\n15. Virtanen P et al. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods [Internet]. 2020;17:261‚Äë72. Disponible sur: https://doi.org/10.1038/s41592-019-0686-2\n\n\n16. Hunter JD. Matplotlib: A 2D Graphics Environment. Computing in Science & Engineering [Internet]. 2007;9(3):90‚Äë5. Disponible sur: https://doi.org/10.1109/MCSE.2007.55\n\n\n17. Waskom ML. Seaborn: Statistical Data Visualization. Journal of Open Source Software [Internet]. 2021;6(60):3021. Disponible sur: https://doi.org/10.21105/joss.03021\n\n\n18. Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research [Internet]. 2011;12:2825‚Äë30. Disponible sur: https://jmlr.org/papers/v12/pedregosa11a.html\n\n\n19. Team JD. Joblib: Running Python Functions as Pipeline Jobs [Internet]. 2020. Disponible sur: https://joblib.readthedocs.io/en/latest/\n\n\n20. Lemaitre G, Walt SJ van der, Passos A. Threadpoolctl: Python Bindings to Control Thread-Pool Behavior in Native Libraries [Internet]. 2022. Disponible sur: https://github.com/joblib/threadpoolctl"
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#annexe-1",
    "href": "projets/missingdatalab/missingdatalab.html#annexe-1",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "10 Annexes 1 : forumulations math√©matiques des donn√©es manquantes",
    "text": "10 Annexes 1 : forumulations math√©matiques des donn√©es manquantes\nDans cette section, nous d√©crivons les diff√©rents m√©canismes de donn√©es manquantes en utilisant des formulations math√©matiques pour clarifier les conditions sous lesquelles ces m√©canismes se produisent. Ces formulations reposent sur la probabilit√© conditionnelle, not√©e \\(\\mathbb{P}\\), o√π \\(M\\) repr√©sente l‚Äôindicateur de donn√©e manquante (avec \\(M = 1\\) si la donn√©e est manquante et \\(M = 0\\) sinon) et \\(X\\) d√©signe l‚Äôensemble des variables observ√©es ou non observ√©es. Soit \\(n\\) le nombre d‚Äôobservations (lignes) et \\(p\\) le nombre de variables (colonnes). On note \\(X \\in \\mathbb{R}^{n\\times p}, \\ M = \\{0,1\\}^{n\\times p}\\) avec \\(1\\) pour NA et \\(0\\) sinon.\nNous utilisons les notations suivantes : \\(X_{obs}\\) les valeurs observ√©es dans le jeu de donn√©es et \\(X_{miss}\\) les valeurs manquantes dans le jeu de donn√©es.\n\n10.1 Donn√©es MCAR\n\\(\\mathbb{P}(M=1\\mid X) = \\mathbb{P}(M=1)\\)\n\n\n10.2 Donn√©es MAR\n\\(\\forall \\ X_{miss}, \\ \\mathbb{P}(M=1 \\mid X) = \\mathbb{P}(M=1 \\mid X_{obs})\\)\n\n\n10.3 Donn√©es MNAR\n\\(\\mathbb{P}(M=1 \\mid X) = \\mathbb{P}(M=1 \\mid X_{obs}, \\ X_{miss})\\)\n\n\n10.4 Probabilit√© d‚Äôabscence\nOn note \\(i\\) la \\(i\\)-i√®me observation et \\(j\\) la \\(j\\)-i√®me variable :\n\\[\n\\forall \\ (i, j) \\in \\{1, \\dots, n\\} \\times \\{1, \\dots, p\\}, \\ \\mathbb{P}(M_{ij} = 1 \\mid Z, Y) = \\sigma(\\alpha_j + Z_i^\\top . \\beta_j + \\gamma_j . Y_i)\n\\]\nAvec :\n\n\\(Z\\) la matrice des covariables \\(X\\) excluant \\(Y\\)\n\\(Y\\) le vecteur des valeurs de la variable cible\n\\(\\alpha_j\\), \\(\\beta_j\\) et \\(\\gamma_j\\) sont les param√®tres d‚Äôajustement propres √† la variable \\(j\\)\n\\(\\forall \\ z \\in \\mathbb{R}, \\ \\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\n\nL‚Äôintercept \\(\\alpha\\) ajuste globalement la proportion de donn√©es manquantes. En le modifiant, on peut augmenter ou r√©duire le taux global de valeurs manquantes. Les poids \\(\\beta\\) d√©terminent l‚Äôimpact de chaque pr√©dicteur sur la probabilit√© de donn√©es manquantes. Un poids √©lev√© signifie une influence importante de la variable correspondante. Le coefficient \\(\\gamma\\) traduit l‚Äôinfluence de la valeur \\(Y_i\\) sur la probabilit√© d‚Äôabsence.\n\n\n10.5 Optimisation\nPour atteintre une proportion cible \\(\\pi_{\\text{target}}\\) de donn√©es manquantes on cherche √† \\(j\\) fix√© :\n\\[\n\\underset{(\\mathcal{A},\\, \\mathcal{B},\\, \\mathcal{G})}{\\arg\\min\\;} \\ \\bigg| \\frac{1}{n} \\sum_{k=1}^n  \\sigma(\\mathcal{A} + Z_k^\\top \\mathcal{B} + \\mathcal{G} Y_k) - \\pi_{\\text{target}} \\bigg|\n\\]\nSi \\(\\mathcal{B} = 0\\) et \\(\\mathcal{G} = 0\\), le m√©canisme est MCAR Si \\(\\mathcal{B} \\neq 0\\) et \\(\\mathcal{G} = 0\\), le m√©canisme est MAR Si \\(\\mathcal{B} \\neq 0\\) et \\(\\mathcal{G} \\neq 0\\), le m√©canisme est MNAR"
  },
  {
    "objectID": "projets/missingdatalab/missingdatalab.html#annexe-2",
    "href": "projets/missingdatalab/missingdatalab.html#annexe-2",
    "title": "MissingDataLab - Simulation et imputation de donn√©es manquantes",
    "section": "11 Annexe 2 : formulations math√©matiques des imputations",
    "text": "11 Annexe 2 : formulations math√©matiques des imputations\nSoit \\(x_j\\) la valeur de la variable \\(X\\) pour l‚Äôobservation \\(j\\). L‚Äôobservation \\(i\\) est imput√©e par \\(\\hat{x}_i\\) pour g√©n√©rer une variable sans donn√©es manquantes \\(\\hat{X}\\).\n\n11.1 Simple\n\nLa moyenne : \\(\\hat{x}_i = \\frac{1}{n} \\sum_{j=1}^{n} x_j\\)\nLa m√©diane : \\(\\hat{x}_i = \\text{mediane}(x_j)\\)\nLe mode : \\(\\hat{x}_i = \\text{mode}(x_j)\\)\n\n\n\n11.2 KNN\nSoit \\(k\\) le nombre de voisins : \\(\\hat{x}_i = \\frac{1}{k} \\sum_{j=1}^{k} x_{j}\\)\n\n\n11.3 SoftImputer\nL‚Äôalgorithme minimise l‚Äôobjectif suivant : \\(\\min_{X} \\frac{1}{2} \\|P_\\Omega(M - X)\\|_F^2 + \\lambda \\|X\\|_*\\)\no√π \\(P_\\Omega\\) est l‚Äôop√©rateur de projection sur les entr√©es observ√©es, \\(\\|\\cdot\\|_F\\) est la norme de Frobenius, et \\(\\|\\cdot\\|_*\\) est la norme nucl√©aire (somme des valeurs singuli√®res).\nL‚Äôalgorithme suit les √©tapes suivantes :\n\nInitialisation en rempla√ßant les valeurs manquantes par la moyenne des colonnes\nApplication d‚Äôune d√©composition en valeurs singuli√®res (SVD) : \\(X = U \\Sigma V^T\\)\nApplication du seuillage doux aux valeurs singuli√®res : \\(\\tilde{\\sigma}_i = \\max(0, \\sigma_i - \\lambda)\\)\nReconstruction de la matrice imput√©e : \\(\\hat{X} = U \\tilde{\\Sigma} V^T\\)\nR√©imposition des valeurs observ√©es sur la matrice reconstruite : \\(\\hat{X}_{ij} = X_{ij}\\)\nR√©p√©tition jusqu‚Äô√† convergence selon un crit√®re \\(\\epsilon\\) bas√© sur la norme de Frobenius : \\(\\|\\hat{X}_{(t+1)} - \\hat{X}_{(t)})\\|_F &lt; \\epsilon\\)\n\n\\(\\lambda\\) et \\(\\epsilon\\) √©tant des hyperparam√®tres.\n\n\n11.4 ACP\nL‚Äôalgorithme suit les √©tapes suivantes :\n\nImputation initiale par la moyenne ou le mode\nApplication d‚Äôune ACP sur les donn√©es\nImputation selon le mod√®le ACP\nIt√©ration jusqu‚Äô√† convergence\n\n\n\n11.5 ICE\nSoit :\n\n\\(X_1, X_2, \\dots, X_p\\) le vecteur des variables explicatives utilis√©es pour l‚Äôimputation\n\\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) les coefficients de r√©gression estim√©s\n\\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\) le terme d‚Äôerreur de variance \\(\\sigma^2\\)\n\\(k\\) une cat√©gorie sp√©cifique de la variable √† imputer\n\\(K\\) la cat√©gorie de r√©f√©rence dans la r√©gression logistique\n\n\nL‚Äôalgorithme commence par une imputation initiale des valeurs manquantes √† l‚Äôaide d‚Äôune m√©thode simple (moyenne ou mode) pour obtenir \\(\\hat{X}\\)\nL‚Äôimputation est ensuite affin√©e par r√©gression, selon la nature de \\(\\hat{X}\\)\n\nPour une variable continue : r√©gression lin√©aire\\(\\hat{x}_{i} = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij} + \\epsilon_i\\)\nPour une variable cat√©gorielle : r√©gression logistique multinomiale\\(\\log\\left(\\frac{P(\\hat{x}_{i} = k)}{P(x_i = K)}\\right) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_{ij}\\)\n\nApr√®s la premi√®re estimation (dans notre cas ICE), l‚Äôalgorithme proc√®de √† une imputation it√©rative pour am√©liorer la pr√©cision. Ainsi, √† chaque it√©ration, les estimations deviennent plus pr√©cises en tenant compte des interactions entre les variables.\n\n\n\n11.6 MissForest\n\nInitialisation par des valeurs simples (moyenne, mode)\nUtilisation de for√™ts al√©atoires pour pr√©dire les valeurs manquantes √† partir des autres variables\nL‚Äôimputation est it√©rative jusqu‚Äô√† convergence"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Nom : \nEmail : \nMessage :\n\n\nEnvoyer"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Accueil",
    "section": "",
    "text": "üí° Curieux de nature et passionn√© d‚Äôinformatique, j‚Äôai d√©but√© l‚Äôalgorithmique avec Python, avant de m‚Äôorienter vers le d√©veloppement web en apprenant SQL et PHP. Pendant mon doctorat en m√©decine v√©t√©rinaire, j‚Äôai d√©couvert les statistiques et l‚Äôanalyse de donn√©es avec R.\nüîç En tant que v√©t√©rinaire praticien lib√©ral pendant 5 ans, j‚Äôai d√©velopp√© une expertise transversale, combinant analyse m√©dicale, capacit√© de diagnostic rapide et sens de la communication. La rigueur n√©cessaire pour g√©rer des situations d‚Äôurgence, synth√©tiser des informations complexes et prendre des d√©cisions m‚Äôa dot√© d‚Äôune r√©silience et d‚Äôune discipline solides. Cette exp√©rience m‚Äôa √©galement appris √† comprendre les besoins des clients et √† y r√©pondre avec des solutions efficaces et bien fond√©es. Dans la data science, il est crucial d‚Äôallier comp√©tences analytiques et capacit√© √† communiquer des r√©sultats clairs et fiables.\nüéì Suite √† une r√©flexion professionnelle approfondie, j‚Äôai entrepris un Master en Statistique et Sciences des Donn√©es (SSD) - Core AI MIAI √† l‚ÄôUniversit√© Grenoble Alpes. J‚Äôai d√©velopp√© mes comp√©tences en statistiques avanc√©es, machine learning, mod√©lisation et exploration de donn√©es textuelles et spatiales. Ce parcours me permet de ma√Ætriser des outils comme R et Python pour concevoir des solutions bas√©es sur l‚ÄôIA. Mon objectif est d‚Äôint√©grer ces comp√©tences √† mon exp√©rience de terrain pour d√©velopper des approches innovantes en data science.\n\n\nExp√©riences professionnelles\n\nStage de recherche ‚Äì Analyse statistique et mod√©lisation de donn√©es\nInstitut de recherche IGE, Grenoble (38) - De mars 2025 √† ao√ªt 2025\n\n\nDocteur en m√©decine v√©t√©rinaire pour animaux de compagnie et animaux d‚Äô√©levage\nClinique v√©t√©rinaire des D√¥mes, Le Broc (63) - De septembre 2021 √† ao√ªt 2024\n\nR√©solution de probl√®mes vari√©s avec une capacit√© d‚Äôadaptation aux situations\n√âtablir des pr√©visions, des √©valuations, des recommandations, des perspectives\n\n\n\nDocteur en m√©decine v√©t√©rinaire pour animaux d‚Äô√©levage\nClinique Ani-M√©dic, La Tardi√®re (85) - De septembre 2019 √† septembre 2021\n\nForce d‚Äôinitiatives et de propositions en animant une d√©marche avec informations et conseils\nTenir une veille scientifique\n\n\n\nStage de recherche ‚Äì Analyse statistique et mod√©lisation de donn√©es\nInstitut de recherche INRA, Le Rheu (35) - De mars 2019 √† juillet 2019\n\nMention tr√®s honorable avec f√©licitations du jury et proposition pour un prix de th√®se\nUtilisation de m√©thodes statistiques avanc√©es (ANOVA, ACP, mod√®les lin√©aires √† effets mixtes)\nTravail collaboratif avec des chercheurs sur la conception et l‚Äôinterpr√©tation des r√©sultats\nPr√©sentation scientifique orale et √©crite des travaux de recherche\n\n\n\nCampagne de prophylaxie v√©t√©rinaire\nClinique Ani-M√©dic, La Tardi√®re (85) - Janvier 2019\n\n\n\n\nDipl√¥mes et formations\n\nMaster 2 Math√©matiques et applications - Parcours statistique et sciences des donn√©es (SSD)\nUniversit√© Grenoble Alpes (UGA), Grenoble (38) - Depuis septembre 2024\n\nStatistiques : tests statistiques, estimation statistique (moments, vraisemblance), r√©gressions (lin√©aire, logistique), GLM, statistique computationnelle (bootstrap, permutation), statistiques en grande dimension (FWER, FDF, m√©thodes de p√©nalisation Lasso et Ridge), biostatistique (mod√®les mixtes, de survie)\nExploration de donn√©es : num√©riques (analyse descriptive), fouille de texte (DL, Word2Vec, NLP, BERT, Hugging Face Transformers), spatiales (krigeage)\nMod√©lisation : bay√©sienne (Monte Carlo), √©chantillonnage, s√©ries temporelles (ARIMA, GARCH)\nMachine Learning : apprentissage supervis√© (classification et r√©gression : K-NN, SVM, Random Forests), non supervis√© (clustering K-means et dimensionnalit√© ACP), apprentissage profond (r√©seaux de neurones CNN, RNN et transformers)\nProgrammation et optimisation (solveurs) :\n\nR : boot, lme4, randomForest, xgboost, ggplot2, RMarkdown\nPython : NumPy, pandas, SciPy, scikit-learn, TensorFlow, Keras, matplotlib, CVXPY\n\n\n\n\nLaur√©at de l‚ÄôAcad√©mie V√©t√©rinaire de France - Prix de th√®se et m√©daille\nAcad√©mie V√©t√©rinaire de France, Paris (75) - Novembre 2020\n\n\nDipl√¥me d‚Äô√âtat de docteur v√©t√©rinaire\nEcole Nationale V√©t√©rinaire, Agroalimentaire et de l‚ÄôAlimentation, Nantes (44) - De septembre 2014 √† septembre 2019\n\n\nClasses pr√©paratoires aux Grandes √âcoles (CPGE)\nLyc√©e Clemenceau, Nantes (44) - De septembre 2011 √† juillet 2014\n\n\n\n\nPublications\n\nGuinard-Flament J et al. Variations in milk lactose content and the mechanisms underlying in dairy cows. ADSA annual meeting 2023. American Dairy Science Association. 2023.\nPDF | hal-04419956\nDecoopman N, Guinard- Flament J et Resmond R. Lactose sanguin et lactose du lait : caract√©risation de leurs facteurs de variation chez la vache laiti√®re. Bull. Acad. V√©t. France. 2022.\nPDF | 10.3406/bavf.2022.70978\nGuinard-Flament J, Decoopman N et Resmond R. Taux de lactose du lait et lactos√©mie chez la vache laiti√®re. PEGASE - Physiologie, Environnement et G√©n√©tique pour l‚ÄôAnimal et les Syst√®mes d‚ÄôElevage. D√©fis scientifiques Phase. Rennes. 2019.\nPDF | hal-02737600\nDecoopman N. Contribution √† la compr√©hension des variations de la lactos√©mie chez la vache laiti√®re. Th√®se pour le dipl√¥me d‚Äô√âtat de Docteur V√©t√©rinaire, Facult√© de m√©decine, Nantes. 2019 ; 125 pages.\nPDF | Doc-ONIRIS\n\n\n\n\nLoisirs\nSi je ne suis pas derri√®re un ordinateur, je suis probablement parti en montagne ! ‚õ∞Ô∏èü•æüöµ‚Äç‚ôÇÔ∏èüßó‚Äç‚ôÇÔ∏è‚õ∑Ô∏èü™Ç"
  }
]